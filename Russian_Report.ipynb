{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проект: RND Exploratory Behavior\n",
    "\n",
    "**Команда: Сергей Червонцев, Иван Провилков**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Важно - при запуске этого ноутбука при использовании конды нужно поменять ядро, см https://stackoverflow.com/a/44786736)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы исследуем exploration поведение агента используя \"Random Network Distillation (RND)\" (https://arxiv.org/abs/1810.12894). \n",
    "\n",
    "Интересно узнать, как ведет себя агент не имея награды от среды, когда он пользуется только своим внутренним \"интересом\", который обеспечивается с помощью RND. Меньше ли получается итоговое качество, сколько времени занимает такое обучение и какие факторы на него влияют?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мы поставили себе задачу написать агента, воспроизвести результаты статьи про RND на игре \"Montezuma revenge\" и посмотреть на поведение агента без награды от среды. \n",
    "\n",
    "### Мы обнаружили, что данная среда и алгоритм работают очень долго. В дальнейшем (после сдачи проекта) мы собираемся продолжать эксперименты с этой средой. Поэтому одной из своих главных задач на проект мы сделали ускорение обучения алгоритма в этой среде. Чтобы эксперименты занимали меньше времени на имеющихся ресурсах (2 сервера по 2 1080ti).\n",
    "\n",
    "### Описание среды:\n",
    "\n",
    "Montezuma revenge это игра Atari, в которой агент должен пройти через множество комнат (всего их 99) в лабиринте, чтобы найти сокровище. Определенные комнаты закрыты дверями и чтобы туда попасть нужно найти и подобрать ключ. В комнатах есть монстры, которые убивают персонажа и их нужно избегать. Также персонаж умирает от падения с высоты или попадания в ловушку (падение в лаву). В комнатах есть различные алмазы, которые игрок может подбирать и получить награду, а также снаряжение, которое можно надеть (факелы, мечи...). У персонажа есть 5 жизней, то есть он может 5 раз умереть до начала новой игры.\n",
    "\n",
    "Состояния среды: RGB Картинка 210x160 пикселей (мы рескейлим до 84х84)\n",
    "\n",
    "Пространство действий: агент может бегать в разные стороны, прыгать и использовать предметы, всего 18 Discrete действий. Полное описание действий можно найти  здесь (https://github.com/openai/gym/blob/2ec4881c22b129d1f06173d136529477c0d8d975/gym/envs/atari/atari_env.py#L219)\n",
    "\n",
    "Extrinsic Reward: даётся за подбор ключа, открытие двери, подбор алмазов в комнатах, убийство монстров оружием.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель\n",
    "\n",
    "Мы воспроизводим модель из статьи \"Random Network Distillation (RND)\". Используем Pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гиперпараметры:\n",
    "\n",
    "Все гиперпараметры эксперимента можно посмотреть в config.yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvName: MontezumaRevengeNoFrameskip-v4\r\n",
      "MaxStepsPerEpisode: 4500\r\n",
      "ImageHeight: 84\r\n",
      "ImageWidth: 84\r\n",
      "UseStickyAction: True\r\n",
      "StickyActionProb: 0.25\r\n",
      "\r\n",
      "NumWorkers: 128\r\n",
      "RolloutSteps: 128\r\n",
      "NumInitSteps: 2048\r\n",
      "\r\n",
      "ExtRewardDiscount: 0.999\r\n",
      "IntRewardDiscount: 0.99\r\n",
      "ExtCoeff: 2.0 # Set this to 0 for intrinsic-only exploration\r\n",
      "IntCoeff: 1.0\r\n",
      "LearningRate: 0.0001\r\n",
      "ClipGradNorm: 0.5\r\n",
      "BatchSize: 4096 # It's recommended to adjust it s.t. NumWorkers * RolloutSteps / BatchSize = 4\r\n",
      "EpochSteps: 4\r\n",
      "SavePath: $SET_PATH_TO_.CKPT_FILE # Example: \"/home/chervontsev/checkpoints/RndAgentFaster.ckpt\"\r\n",
      "# Add StateDict argument to continue training from checkpoint\r\n",
      "\r\n",
      "NumEpochs: 10000\r\n",
      "\r\n",
      "RNDUpdateProportion: 0.25\r\n",
      "PPOEntropyCoeff: 0.001\r\n",
      "PPORewardEps: 0.1\r\n",
      "PPOAdvLambda: 0.95\r\n",
      "UseVTraceCorrection: False\r\n",
      "\r\n",
      "OptimDevice: 'cuda:0'\r\n",
      "RunDevice: 'cuda:0'\r\n",
      "UseTPU: False\r\n"
     ]
    }
   ],
   "source": [
    "!cat config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Некоторые гиперпараметры:\n",
    "\n",
    "UseStickyAction: True -- использовать ли повторение предыдущего действия\n",
    "\n",
    "StickyActionProb: 0.25 -- вероятность того, что повторится предыдущее действие\n",
    "\n",
    "NumWorkers: 128 -- количество воркеров (процессов) в которых параллельно будет происходить сбор статистик для обучения. \n",
    "\n",
    "RolloutSteps: 128 -- количество шагов делаемых каждым воркером для сбора статистики\n",
    "\n",
    "NumInitSteps: 1024 -- количество стартовых шагов.\n",
    "\n",
    "ExtCoeff (extrinsic coefficient): -- коэффициент отвечающий за то как сильно мы учитываем награду от среды.\n",
    "\n",
    "IntCoeff (intrinsic coefficient): -- коэффициент отвечающий за то как сильно мы учитываем внутреннюю награду (Random Network).\n",
    "\n",
    "UseVTraceCorrection: -- использовать ли V-Trace\n",
    "\n",
    "UseTPU -- в разработке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результаты запусков:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Ext Coeff: 2.0, Int Coeff 1.0\n",
    "\n",
    "**ExtRew per Rollout:**\n",
    "\n",
    "<img src=\"log_screens/usual_2_reward_per_rollout.png\" />\n",
    "\n",
    "**IntRew per Rollout:**\n",
    "\n",
    "<img src=\"log_screens/usual_2_int_reward_per_rollout.png\" />\n",
    "\n",
    "**ExtRew per Episode (on one of 128 workers):**\n",
    "\n",
    "<img src=\"log_screens/usual_2_reward_per_epi.png\" />\n",
    "\n",
    "**Example of work (checkpoint from 5k steps):**\n",
    "\n",
    "<img src=\"videos/usual_training_2.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtCoeff: 1.0, IntCoeff: 1.0\n",
    "#### (Понижение влияния Extrinsic Reward)\n",
    "\n",
    "**ExtRew per Rollout:**\n",
    "<img src=\"log_screens/usual_1_reward_per_rollout.png\" />\n",
    "\n",
    "**IntRew per Rollout:**\n",
    "<img src=\"log_screens/usual_1_int_reward_per_rollout.png\" />\n",
    "\n",
    "**ExtRew per Episode:**\n",
    "<img src=\"log_screens/usual_1_reward_per_epi.png\" />\n",
    "\n",
    "**Example of work (checkpoint from 10k steps):**\n",
    "<img src=\"videos/usual_1.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtCoeff 0.0, IntCoeff 1.0\n",
    "\n",
    "**ExtRew per Rollout:**\n",
    "<img src=\"log_screens/only_i_reward_per_rollout.png\" />\n",
    "\n",
    "**IntRew per Rollout:**\n",
    "<img src=\"log_screens/only_i_int_reward_per_rollout.png\" />\n",
    "\n",
    "**ExtRew per Episode (model does not train on it):**\n",
    "<img src=\"log_screens/only_i_reward_per_epi.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Чекпоинт с этого запуска не сохранился, поэтому без видео**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Видим, что когда модель не наблюдает награду от среды, то эта награда увеличивается очень медленно (модель никто не поощряет за взятие алмазов). Также интересно то, что Intrinsic Reward после 3000 шагов начинает в среднем медленно расти. То есть модель понемногу учится исследовать что-то новое. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ускорение\n",
    "\n",
    "#### В базовой модели, которую мы написали, обучение на 10.000 шагов занимает около 3х дней. Причем дальнейшее ускорение вширь (увеличивая количество cpu или gpu) сделать трудно, так как увеличение NWorkers не ускоряет обучение моделей, как и увеличение размера сети или батча. Мы замерили, что в такой реализации на работу сбора статистик в environment-е тратится около 60% времени работы основного процесса (при том что этот сбор делается сразу 128 воркерами). В это время обучение сети на ГПУ не происходит. Мы решили распараллелить два этих процесса, чтобы обучение происходило одновременно со сбором статистик на старых (отстающих на 1 апдейт) весах сети (как в IMPALA, но не совсем - мы синхронизируем actor-а после каждого степа, чтобы считать следующую шаги быстро на гпу).\n",
    "\n",
    "**Схема работы:**\n",
    "\n",
    "<img src=\"profiling/parallel_scheme.png\" width=\"800\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результаты ускорения:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtCoeff 0.0, IntCoeff 1.0 (parallel env and training)\n",
    "\n",
    "<img src=\"log_screens/usual_2_reward_per_rollout.png\" />\n",
    "\n",
    "**ExtRew per Rollout:**\n",
    "<img src=\"log_screens/only_int_fast_reward_per_rollout.png\" />\n",
    "\n",
    "**IntRew per Rollout:**\n",
    "<img src=\"log_screens/only_int_fast_int_reward_per_rollout.png\" />\n",
    "\n",
    "**ExtRew per Episode (model does not train on it):**\n",
    "<img src=\"log_screens/only_int_fast_reward_per_epi.png\" />\n",
    "\n",
    "**Example of work:**\n",
    "<img src=\"videos/only_intrinsic_fast.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**На этом видео агент научился долго не умирать. При этом интересно, что он заходит во вторую комнату и начинает \"играться с летающими черепами, а как-только их там не становится, то больше он туда не хочет заходить. Подобную игру с черепами можно наблюдать и на видео с ExtRew 1, IntRew 1 выше. Скорее всего это происходит потому, что прыгая вокруг черепов он постоянно получает новые картинки, к которым сеть еще не привыкла. За эти новые картинки intrinsic reward награждает его, так как агент еще не успел выучить ответ случайной сети на них.**\n",
    "\n",
    "**Также, судя по графикам обучения, то, что мы делаем подсчет статистик на старых (отстающих на 1 апдейт) весах влияет на обучение, потому что за то же количество эпох агент достиг меньшей награды и продвинулся не так далеко.**\n",
    "\n",
    "**Для того чтобы с этим побороться мы решили добавить V-Trace, однако в наших запусках с ним пока получалось только хуже. Почему так происходит, мы пока не поняли.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результаты ускорения.\n",
    "\n",
    "Результаты обучения на 10к шагов, 128 воркеров (более подробные графы с временами вычисления см в profiling)\n",
    "\n",
    "| Модель |  Время (часов)  | Coeff |\n",
    "|------|------|------|\n",
    "|  Базовая модель| 84| x1|\n",
    "|  Модель с раздельным env_runner и trainer| 33| x2.5|\n",
    "\n",
    "Таким образом, мы смогли получить ускорение в 2.5 раза."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нам удалось воспроизвести статью Random Network Distillation.**\n",
    "\n",
    "**Наши эксперименты показали, что с помощью RND агент может учиться без награды от среды. Однако, это занимает больше времени, а также ему труднее попасть в новые комнаты.**\n",
    "\n",
    "**Мы заметили, что из-за RND награды агент может бегать вокруг движущихся объектов (черепов). Картинка меняется и агент получает немного больший ревард. С этой проблемой как-то надо бороться. В случае с движущимися монстрами, агент умирает от своей беготни вокруг монстров. Поэтому можно добавить отрицательный ревард за потерю жизни.**\n",
    "\n",
    "**Мы реализовали параллельное выполнение env_runner-а, который собирает статистики, и процесса обновления весов. Это дало нам ускорение в 2.5 раза.**\n",
    "\n",
    "**Мы реализовали V-Trace, однако результаты с ним у нас пока были хуже, чем без него. Проблему пока не нашли.**\n",
    "\n",
    "### Планы на будущее\n",
    "\n",
    "**Понять в чем проблема с V-Trace**\n",
    "\n",
    "**Улучшить алгоритм обучения в данной среде за счет добавления других способов предсказания uncertainty агента для действий и наблюдений.**\n",
    "\n",
    "**Добавить поддержку TPU, учитывающую особенности его работы (синхронизацию ядер после каждого backward pass-а, сейчас разбираемся [в этом issue](https://github.com/pytorch/xla/issues/2103)).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инструкции по запуску кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для запуска модели нужно установить нужные библиотеки, мы используем python3.6 и cuda9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "CondaValueError: prefix already exists: /home/jan/miniconda3/envs/rnd\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!conda env create -f requirements_gpu.yml # или pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запуск агента с обученными весами\n",
    "\n",
    "1. Скачиваем веса из google drive с помощью wget (ниже) или руками: https://drive.google.com/drive/folders/15RBD-BrWUlylYLR13u7NWRhnMTAb9cWs?usp=sharing\n",
    "2. Пишем путь до весов в config.yaml. \n",
    "Пример: SavePath: \"/home/chervontsev/checkpoints/usual_2.ckpt\", выбираем надо ли оставлять UseStickyAction, мы по умолчанию оставляли. \n",
    "3. Запускаем eval.py скрипт: python eval.py. В логах скрипта будет: шаг с которого взят чекпоинт, ревард среды за игру, список посещенных комнат.\n",
    "4. Видео игры агента появится в папке MontezumaRevengeNoFrameskip-v4_example_run в корне проекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usual_2.ckpt\n",
    "!mkdir checkpoints\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1XyO39st3QnCg5NnQ__OTH-ohwWA93YAx' -O checkpoints/usual_2.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usual_1.ckpt\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1WdABk7j7Bs-1_rdp7wU6mfy6gnv5M0UO'\\\n",
    "    -O checkpoints/usual_1.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only_intrinsic_fast.ckpt\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=16UteZvE_4bcciNiCB4jK7EG_h1crHLeo'\\\n",
    "    -O checkpoints/only_intrinsic_fast.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/miniconda3/envs/rnd/lib/python3.6/site-packages/ipykernel_launcher.py:5: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Change config path\n",
    "import yaml\n",
    "\n",
    "with open(\"config.yaml\") as f:\n",
    "    current_config = yaml.load(f)\n",
    "\n",
    "current_config[\"SavePath\"] = \"checkpoints/usual_2.ckpt\"\n",
    "\n",
    "with open(\"config.yaml\", \"w\") as f:\n",
    "    yaml.dump(current_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_updates 5000\n",
      "Finished, total reward is 4600\n",
      "All visited rooms: {0, 1, 4, 10, 11, 18, 19, 20}\n"
     ]
    }
   ],
   "source": [
    "!python montezuma_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запуск обучения:\n",
    "\n",
    "1) Настраиваем config.yaml\n",
    "\n",
    "2) Запускаем обучение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python montezuma_train.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rnd)",
   "language": "python",
   "name": "rnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}