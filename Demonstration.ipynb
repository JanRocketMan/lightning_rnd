{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проект: RND Exploratory Behavior\n",
    "\n",
    "**Команда: Сергей Червонцев, Иван Провилков**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы исследуем exploration поведение агента используя \"Random Network Distillation (RND)\" (https://arxiv.org/abs/1810.12894). \n",
    "\n",
    "Интересно узнать, как ведет себя агент не имея награды от среды, когда он пользуется только своим внутренним \"интересом\", который обеспечивается с помощью RND. Меньше ли получается итоговое качество, сколько времени занимает такое обучение и какие факторы на него влияют?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мы поставили себе задачу написать агента, воспроизвести результаты статьи про RND на игре \"Montezuma revenge\" и посмотреть на поведение агента без награды от среды. \n",
    "\n",
    "### Мы обнаружили, что данная среда и алгоритм работают очень долго. В дальнейшем (после сдачи проекта) мы собираемся продолжать эксперименты с этой средой. Поэтому одной из своих главных задач на проект мы сделали ускорение обучения алгоритма в этой среде. Чтобы эксперименты занимали разумное время на наших ресурсах (2 сервера по 2 1080ti).\n",
    "\n",
    "### Описание среды:\n",
    "\n",
    "Montezuma revenge это игра Atari, в которой агент должен пройти через множество комнат (всего их 99) в лабиринте, чтобы найти сокровище. Определенные комнаты закрыты дверями и чтобы туда попасть нужно найти и подобрать ключ. В комнатах есть монстры, которые убивают персонажа и их нужно избегать. Также персонаж умирает от падения с высоты или попадания в ловушку (падение в лаву). В комнатах есть различные алмазы, которые игрок может подбирать и получить награду, а также снаряжение, которое можно надеть (факелы, мечи...). У персонажа есть 5 жизней, то есть он может 5 раз умереть до начала новой игры.\n",
    "\n",
    "Состояния среды: RGB Картинка 210x160 пикселей (мы рескейлим до 84х84)\n",
    "\n",
    "Пространство действий: агент может бегать в разные стороны и прыгать, всего 18 Discrete действий.\n",
    "\n",
    "Extrinsic Reward: даётся за подбор ключа, открытие двери, подбор алмазов в комнатах.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель\n",
    "\n",
    "Мы воспроизводим модель из статьи \"Random Network Distillation (RND)\". Используем Pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гиперпараметры:\n",
    "\n",
    "Все гиперпараметры эксперимента можно посмотреть в config.yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvName: MontezumaRevengeNoFrameskip-v4\r\n",
      "MaxStepsPerEpisode: 4500\r\n",
      "ImageHeight: 84\r\n",
      "ImageWidth: 84\r\n",
      "UseStickyAction: True\r\n",
      "StickyActionProb: 0.25\r\n",
      "\r\n",
      "NumWorkers: 128\r\n",
      "RolloutSteps: 128\r\n",
      "NumInitSteps: 1024\r\n",
      "\r\n",
      "ExtRewardDiscount: 0.999\r\n",
      "IntRewardDiscount: 0.99\r\n",
      "ExtCoeff: 2.0\r\n",
      "IntCoeff: 1.0\r\n",
      "LearningRate: 0.0001\r\n",
      "ClipGradNorm: 10.0\r\n",
      "BatchSize: 4096\r\n",
      "EpochSteps: 4\r\n",
      "SavePath: \"/home/chervontsev/checkpoints/RndAgentAlt.ckpt\"\r\n",
      "\r\n",
      "NumEpochs: 10000\r\n",
      "\r\n",
      "RNDUpdateProportion: 0.25\r\n",
      "PPOEntropyCoeff: 0.001\r\n",
      "PPORewardEps: 0.1\r\n",
      "PPOAdvLambda: 0.95\r\n",
      "UseVTraceCorrection: True\r\n",
      "\r\n",
      "UseTPU: False\r\n"
     ]
    }
   ],
   "source": [
    "!cat config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Некоторые гиперпараметры:\n",
    "\n",
    "UseStickyAction: True -- использовать ли повторение предыдущего действия\n",
    "\n",
    "StickyActionProb: 0.25 -- вероятность того, что повторится предыдущее действие\n",
    "\n",
    "NumWorkers: 128 -- количество воркеров (процессов) в которых параллельно будет происходить сбор статистик для обучения. \n",
    "\n",
    "RolloutSteps: 128 -- количество шагов делаемых каждым воркером для сбора статистики\n",
    "\n",
    "NumInitSteps: 1024 -- количество стартовых шагов.\n",
    "\n",
    "ExtCoeff (extrinsic coefficient): -- коэффициент отвечающий за то как сильно мы учитываем награду от среды.\n",
    "\n",
    "IntCoeff (intrinsic coefficient): -- коэффициент отвечающий за то как сильно мы учитываем внутреннюю награду (Random Network).\n",
    "\n",
    "UseVTraceCorrection: -- использовать ли V-Trace\n",
    "\n",
    "UseTPU -- в разработке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результаты запусков:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Ext Coeff: 2.0, Int Coeff 1.0\n",
    "\n",
    "**ExtRew per Rollout:**\n",
    "![](./log_screens/usual_2_reward_per_rollout.png)\n",
    "\n",
    "**IntRew per Rollout:**\n",
    "![](./log_screens/usual_2_int_reward_per_rollout.png)\n",
    "\n",
    "**ExtRew per Episode (we log 1 worker with fixed index, total there are 128 workers):**\n",
    "![](./log_screens/usual_2_reward_per_epi.png)\n",
    "\n",
    "**Example of work (checkpoint from 5k steps):**\n",
    "![](./videos/usual_training_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ExtCoeff: 1.0, IntCoeff: 1.0\n",
    "#### Понижение влияния Extrinsic Reward (награды от среды)\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtCoeff 0.0, IntCoeff 1.0\n",
    "#### Оставим только Intrinsic Reward (RND)\n",
    "\n",
    "**ExtRew per Rollout:**\n",
    "![](./log_screens/only_i_reward_per_rollout.png)\n",
    "\n",
    "**IntRew per Rollout:**\n",
    "![](./log_screens/only_i_int_reward_per_rollout.png)\n",
    "\n",
    "**ExtRew per Episode (model does not train on it):**\n",
    "![](./log_screens/only_i_reward_per_epi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Чекпоинт с этого запуска не сохранился, поэтому без видео**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Видим, что когда модель не наблюдает награду от среды, то эта награда увеличивается очень медленно (модель никто не поощряет за взятие алмазов). Также интересно то, что Intrinsic Reward после 3000 шагов начинает в среднем медленно расти. То есть модель понемногу учится исследовать что-то новое. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ускорение\n",
    "\n",
    "#### В базовой модели, которую мы написали, обучение на 10.000 шагов занимает около 3х дней. Причем дальнейшее ускорение вширь (увеличивая количество cpu или gpu) сделать трудно, так как увеличение NWorkers не ускоряет обучение моделей, как и увеличение размера сети или батча. Мы замерили, что в такой реализации на работу сбора статистик в environment-е тратится около 60% времени работы основного процесса (при том что этот сбор делается сразу 128 воркерами). В это время обучение сети на ГПУ не происходит. Мы решили распараллелить два этих процесса, чтобы обучение происходило одновременно со сбором статистик на старых (отстающих на 1 апдейт) весах сети (как в IMPALA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результаты ускорения:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtCoeff 0.0, IntCoeff 1.0 (parallel env and training)\n",
    "\n",
    "**ExtRew per Rollout:**\n",
    "![](./log_screens/only_int_fast_reward_per_rollout.png)\n",
    "\n",
    "**IntRew per Rollout:**\n",
    "![](./log_screens/only_int_fast_int_reward_per_rollout.png)\n",
    "\n",
    "**ExtRew per Episode (model does not train on it):**\n",
    "![](./log_screens/only_int_fast_reward_per_epi.png)\n",
    "\n",
    "**Example of work:**\n",
    "![](./videos/only_intrinsic_fast.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**На этом видео агент научился долго не умирать. При этом интересно, что он заходит во вторую комнату и начинает \"играться с летающими черепами, а как-только их там не становится, то больше он туда не хочет заходить. Скорее всего это происходит потому, что прыгая вокруг черепов он постоянно получает новые картинки, не те, к которым он уже привык. За эти новые картинки intrinsic reward награждает его, так как агент не часто видел эти состояния и еще не успел выучить ответ случайной сети.**\n",
    "\n",
    "**Также, судя по графикам обучения, то, что мы делаем подсчет статистик на старых (отстающих на 1 апдейт) весах влияет на обучение, потому что за то же время агент достиг меньшей награды.**\n",
    "\n",
    "**Для того чтобы с этим побороться мы решили добавить V-Trace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0+cu92'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инструкции по запуску кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для запуска модели нужно установить нужные библиотеки, мы используем python3.6 и cuda9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установка torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.5.0+cu92 torchvision==0.6.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запуск агента с обученными весами\n",
    "\n",
    "1. Скачиваем веса из google drive с помощью wget (ниже) или руками: https://drive.google.com/drive/folders/15RBD-BrWUlylYLR13u7NWRhnMTAb9cWs?usp=sharing\n",
    "2. Пишем путь до весов в config.yaml. \n",
    "Пример: SavePath: \"/home/chervontsev/checkpoints/usual_2.ckpt\", выбираем надо ли оставлять UseStickyAction, мы по умолчанию оставляли. \n",
    "3. Запускаем eval.py скрипт: python eval.py, в логах скрипта: шаг с которого взят чекпоинт, ревард среды за игру, список посещенных комнат.\n",
    "4. Видео игры агента появится в папке MontezumaRevengeNoFrameskip-v4_example_run в корне проекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-28 21:33:17--  https://docs.google.com/uc?export=download&id=1XyO39st3QnCg5NnQ__OTH-ohwWA93YAx\n",
      "Resolving docs.google.com (docs.google.com)... 2a00:1450:4010:c01::c2, 64.233.162.194\n",
      "Connecting to docs.google.com (docs.google.com)|2a00:1450:4010:c01::c2|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-00-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ddul2fqhs1g7fjud52ah7n359rau0rap/1590690750000/01498080638632719806/*/1XyO39st3QnCg5NnQ__OTH-ohwWA93YAx?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-05-28 21:33:22--  https://doc-00-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ddul2fqhs1g7fjud52ah7n359rau0rap/1590690750000/01498080638632719806/*/1XyO39st3QnCg5NnQ__OTH-ohwWA93YAx?e=download\n",
      "Resolving doc-00-18-docs.googleusercontent.com (doc-00-18-docs.googleusercontent.com)... 2a00:1450:4010:c02::84, 74.125.205.132\n",
      "Connecting to doc-00-18-docs.googleusercontent.com (doc-00-18-docs.googleusercontent.com)|2a00:1450:4010:c02::84|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/octet-stream]\n",
      "Saving to: ‘usual_2.ckpt’\n",
      "\n",
      "usual_2.ckpt            [    <=>             ]  62.05M  76.5MB/s    in 0.8s    \n",
      "\n",
      "2020-05-28 21:33:23 (76.5 MB/s) - ‘usual_2.ckpt’ saved [65066732]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# usual_2.ckpt\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1XyO39st3QnCg5NnQ__OTH-ohwWA93YAx' -O usual_2.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-28 21:42:39--  https://docs.google.com/uc?export=download&id=16UteZvE_4bcciNiCB4jK7EG_h1crHLeo\n",
      "Resolving docs.google.com (docs.google.com)... 2a00:1450:4010:c01::c2, 64.233.162.194\n",
      "Connecting to docs.google.com (docs.google.com)|2a00:1450:4010:c01::c2|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0k-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/q8k7o0c761eha360s6i831rlp319ermp/1590691350000/01498080638632719806/*/16UteZvE_4bcciNiCB4jK7EG_h1crHLeo?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-05-28 21:42:42--  https://doc-0k-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/q8k7o0c761eha360s6i831rlp319ermp/1590691350000/01498080638632719806/*/16UteZvE_4bcciNiCB4jK7EG_h1crHLeo?e=download\n",
      "Resolving doc-0k-18-docs.googleusercontent.com (doc-0k-18-docs.googleusercontent.com)... 2a00:1450:4010:c02::84, 74.125.205.132\n",
      "Connecting to doc-0k-18-docs.googleusercontent.com (doc-0k-18-docs.googleusercontent.com)|2a00:1450:4010:c02::84|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/octet-stream]\n",
      "Saving to: ‘only_intrinsic_fast.ckpt’\n",
      "\n",
      "only_intrinsic_fast     [    <=>             ]  47.40M  70.8MB/s    in 0.7s    \n",
      "\n",
      "2020-05-28 21:42:43 (70.8 MB/s) - ‘only_intrinsic_fast.ckpt’ saved [49702154]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# only_intrinsic_fast.ckpt\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=16UteZvE_4bcciNiCB4jK7EG_h1crHLeo'\\\n",
    "    -O only_intrinsic_fast.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_updates 5000\n",
      "Finished, total reward is 4600\n",
      "All visited rooms: {0, 1, 4, 10, 11, 18, 19, 20}\n"
     ]
    }
   ],
   "source": [
    "!python eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**После удачного запуска агентов, когда мы убедились, что алгоритм работает, мы решили поставить себе задачу ускорить процесс обучения, так как обучение на этой среде занимает очень много времени и у нас не очень много ресурсов.**\n",
    "\n",
    "**Этот процесс мы разделили на несколько этапов:**\n",
    "\n",
    "1. После профилирования мы обнаружили, что много времени (~10 секунд на 1 .accumulate_rollout_steps() со 128 лернерами(воркерами)) занимает подготовка статистик необходимых для шага обучения агента, и примерно столько-же времени занимает сам шаг обучения. То есть ГПУ простаивает половину времени.Чтобы решить эту проблему мы решили сделать раздельное обучение actor-а (который собирает статистики необходимые для работы PPO) и learner-а (который делает апдейты оптимайзером) на примере того как это сделано в IMPALA (Включая V-trace).\n",
    "2. Подумали, что стоит переписать имеющийся код на TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Мы реализовали пункт 1, что дало нам ускорение примерно в 2.3 раза. Результаты находятся в ветке impala (https://github.com/JanRocketMan/lightning_rnd/tree/impala)**\n",
    "\n",
    "**Второй пункт остается на future work**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нам удалось воспроизвести статью Random Network Distillation.**\n",
    "\n",
    "**Наши эксперименты показали, что с помощью RND агент может учиться без награды от среды. Однако, это занимает больше времени, а также ему труднее попасть в новые комнаты, в которые он как-то попал в прошлый раз. (Так как нет сильного дополнительного реварда от среды).**\n",
    "\n",
    "**Мы реализовали оптимизацию, которая разделяет обучение actor-а и learner-а, что дало нам ускорение в 2.3 раза.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
