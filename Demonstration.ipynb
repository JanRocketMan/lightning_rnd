{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проект: RND Exploratory Behavior\n",
    "\n",
    "**Команда: Сергей Червонцев, Иван Провилков**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы исследуем exploration поведение агента используя \"Random Network Distillation (RND)\" (https://arxiv.org/abs/1810.12894). \n",
    "\n",
    "Интересно узнать, как ведет себя агент не имея награды от среды, когда он пользуется только своим внутренним \"интересом\", который обеспечивается с помощью RND. Меньше ли получается итоговое качество, сколько времени занимает такое обучение и какие факторы на него влияют?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мы поставили себе задачу написать агента, воспроизвести результаты статьи про RND на игре \"Montezuma revenge\" и посмотреть на поведение агента без награды от среды. \n",
    "\n",
    "### Мы обнаружили, что данная среда и алгоритм работают очень долго. В дальнейшем (после сдачи проекта) мы собираемся продолжать эксперименты с этой средой. Поэтому одной из своих главных задач на проект мы сделали ускорение обучения алгоритма в этой среде. Чтобы эксперименты занимали меньше времени на имеющихся ресурсах (2 сервера по 2 1080ti).\n",
    "\n",
    "### Описание среды:\n",
    "\n",
    "Montezuma revenge это игра Atari, в которой агент должен пройти через множество комнат (всего их 99) в лабиринте, чтобы найти сокровище. Определенные комнаты закрыты дверями и чтобы туда попасть нужно найти и подобрать ключ. В комнатах есть монстры, которые убивают персонажа и их нужно избегать. Также персонаж умирает от падения с высоты или попадания в ловушку (падение в лаву). В комнатах есть различные алмазы, которые игрок может подбирать и получить награду, а также снаряжение, которое можно надеть (факелы, мечи...). У персонажа есть 5 жизней, то есть он может 5 раз умереть до начала новой игры.\n",
    "\n",
    "Состояния среды: RGB Картинка 210x160 пикселей (мы рескейлим до 84х84)\n",
    "\n",
    "Пространство действий: агент может бегать в разные стороны, прыгать и использовать предметы, всего 18 Discrete действий. Полное описание действий можно найти  здесь (https://github.com/openai/gym/blob/2ec4881c22b129d1f06173d136529477c0d8d975/gym/envs/atari/atari_env.py#L219)\n",
    "\n",
    "Extrinsic Reward: даётся за подбор ключа, открытие двери, подбор алмазов в комнатах, убийство монстров оружием.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель\n",
    "\n",
    "Мы воспроизводим модель из статьи \"Random Network Distillation (RND)\". Используем Pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гиперпараметры:\n",
    "\n",
    "Все гиперпараметры эксперимента можно посмотреть в config.yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvName: MontezumaRevengeNoFrameskip-v4\r\n",
      "MaxStepsPerEpisode: 4500\r\n",
      "ImageHeight: 84\r\n",
      "ImageWidth: 84\r\n",
      "UseStickyAction: True\r\n",
      "StickyActionProb: 0.25\r\n",
      "\r\n",
      "NumWorkers: 128\r\n",
      "RolloutSteps: 128\r\n",
      "NumInitSteps: 2048\r\n",
      "\r\n",
      "ExtRewardDiscount: 0.999\r\n",
      "IntRewardDiscount: 0.99\r\n",
      "ExtCoeff: 2.0\r\n",
      "IntCoeff: 1.0\r\n",
      "LearningRate: 0.0001\r\n",
      "ClipGradNorm: 10.0\r\n",
      "BatchSize: 4096\r\n",
      "EpochSteps: 4\r\n",
      "SavePath: \"/home/iv-provilkov/lightning_rnd/only_intrinsic_fast.ckpt\"\r\n",
      "\r\n",
      "NumEpochs: 10000\r\n",
      "\r\n",
      "RNDUpdateProportion: 0.25\r\n",
      "PPOEntropyCoeff: 0.001\r\n",
      "PPORewardEps: 0.1\r\n",
      "PPOAdvLambda: 0.95\r\n",
      "UseVTraceCorrection: True\r\n",
      "\r\n",
      "UseTPU: False\r\n"
     ]
    }
   ],
   "source": [
    "!cat config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Некоторые гиперпараметры:\n",
    "\n",
    "UseStickyAction: True -- использовать ли повторение предыдущего действия\n",
    "\n",
    "StickyActionProb: 0.25 -- вероятность того, что повторится предыдущее действие\n",
    "\n",
    "NumWorkers: 128 -- количество воркеров (процессов) в которых параллельно будет происходить сбор статистик для обучения. \n",
    "\n",
    "RolloutSteps: 128 -- количество шагов делаемых каждым воркером для сбора статистики\n",
    "\n",
    "NumInitSteps: 1024 -- количество стартовых шагов.\n",
    "\n",
    "ExtCoeff (extrinsic coefficient): -- коэффициент отвечающий за то как сильно мы учитываем награду от среды.\n",
    "\n",
    "IntCoeff (intrinsic coefficient): -- коэффициент отвечающий за то как сильно мы учитываем внутреннюю награду (Random Network).\n",
    "\n",
    "UseVTraceCorrection: -- использовать ли V-Trace\n",
    "\n",
    "UseTPU -- в разработке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результаты запусков:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Ext Coeff: 2.0, Int Coeff 1.0\n",
    "\n",
    "**ExtRew per Rollout:**\n",
    "![](./log_screens/usual_2_reward_per_rollout.png)\n",
    "\n",
    "**IntRew per Rollout:**\n",
    "![](./log_screens/usual_2_int_reward_per_rollout.png)\n",
    "\n",
    "**ExtRew per Episode (we log 1 worker with fixed index, total there are 128 workers):**\n",
    "![](./log_screens/usual_2_reward_per_epi.png)\n",
    "\n",
    "**Example of work (checkpoint from 5k steps):**\n",
    "![](./videos/usual_training_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ExtCoeff: 1.0, IntCoeff: 1.0\n",
    "#### Понижение влияния Extrinsic Reward (награды от среды)\n",
    "\n",
    "**ExtRew per Rollout:**\n",
    "![](./log_screens/usual_1_reward_per_rollout.png)\n",
    "\n",
    "**IntRew per Rollout:**\n",
    "![](./log_screens/usual_1_int_reward_per_rollout.png)\n",
    "\n",
    "**ExtRew per Episode:**\n",
    "![](./log_screens/usual_1_reward_per_epi.png)\n",
    "\n",
    "**Example of work (checkpoint from 10k steps):**\n",
    "![](./videos/usual_1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtCoeff 0.0, IntCoeff 1.0\n",
    "#### Оставим только Intrinsic Reward (RND)\n",
    "\n",
    "**ExtRew per Rollout:**\n",
    "![](./log_screens/only_i_reward_per_rollout.png)\n",
    "\n",
    "**IntRew per Rollout:**\n",
    "![](./log_screens/only_i_int_reward_per_rollout.png)\n",
    "\n",
    "**ExtRew per Episode (model does not train on it):**\n",
    "![](./log_screens/only_i_reward_per_epi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Чекпоинт с этого запуска не сохранился, поэтому без видео**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Видим, что когда модель не наблюдает награду от среды, то эта награда увеличивается очень медленно (модель никто не поощряет за взятие алмазов). Также интересно то, что Intrinsic Reward после 3000 шагов начинает в среднем медленно расти. То есть модель понемногу учится исследовать что-то новое. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ускорение\n",
    "\n",
    "#### В базовой модели, которую мы написали, обучение на 10.000 шагов занимает около 3х дней. Причем дальнейшее ускорение вширь (увеличивая количество cpu или gpu) сделать трудно, так как увеличение NWorkers не ускоряет обучение моделей, как и увеличение размера сети или батча. Мы замерили, что в такой реализации на работу сбора статистик в environment-е тратится около 60% времени работы основного процесса (при том что этот сбор делается сразу 128 воркерами). В это время обучение сети на ГПУ не происходит. Мы решили распараллелить два этих процесса, чтобы обучение происходило одновременно со сбором статистик на старых (отстающих на 1 апдейт) весах сети (как в IMPALA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результаты ускорения:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtCoeff 0.0, IntCoeff 1.0 (parallel env and training)\n",
    "\n",
    "**ExtRew per Rollout:**\n",
    "![](./log_screens/only_int_fast_reward_per_rollout.png)\n",
    "\n",
    "**IntRew per Rollout:**\n",
    "![](./log_screens/only_int_fast_int_reward_per_rollout.png)\n",
    "\n",
    "**ExtRew per Episode (model does not train on it):**\n",
    "![](./log_screens/only_int_fast_reward_per_epi.png)\n",
    "\n",
    "**Example of work:**\n",
    "![](./videos/only_intrinsic_fast.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**На этом видео агент научился долго не умирать. При этом интересно, что он заходит во вторую комнату и начинает \"играться с летающими черепами, а как-только их там не становится, то больше он туда не хочет заходить. Подобную игру с черепами можно наблюдать и на видео с ExtRew 1, IntRew 1 выше. Скорее всего это происходит потому, что прыгая вокруг черепов он постоянно получает новые картинки, к которым сеть еще не привыкла. За эти новые картинки intrinsic reward награждает его, так как агент еще не успел выучить ответ случайной сети на них.**\n",
    "\n",
    "**Также, судя по графикам обучения, то, что мы делаем подсчет статистик на старых (отстающих на 1 апдейт) весах влияет на обучение, потому что за то же количество эпох агент достиг меньшей награды и продвинулся не так далеко.**\n",
    "\n",
    "**Для того чтобы с этим побороться мы решили добавить V-Trace, однако в наших запусках с ним пока получалось только хуже. Почему так происходит, мы пока не поняли.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результаты ускорения.\n",
    "\n",
    "Результаты обучения на 10к шагов, 128 воркеров.\n",
    "\n",
    "| Модель |  Время (часов)  | Coeff |\n",
    "|------|------|------|\n",
    "|  Базовая модель| 84| x1|\n",
    "|  Модель с раздельным env_runner и trainer| 33| x2.5|\n",
    "\n",
    "Таким образом, мы смогли получить ускорение в 2.5 раза."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нам удалось воспроизвести статью Random Network Distillation.**\n",
    "\n",
    "**Наши эксперименты показали, что с помощью RND агент может учиться без награды от среды. Однако, это занимает больше времени, а также ему труднее попасть в новые комнаты.**\n",
    "\n",
    "**Мы заметили, что из-за RND награды агент может бегать вокруг движущихся объектов (черепов). Картинка меняется и агент получает немного больший ревард. С этой проблемой как-то надо бороться. В случае с движущимися монстрами, агент умирает от своей беготни вокруг монстров. Поэтому можно добавить отрицательный ревард за потерю жизни.**\n",
    "\n",
    "**Мы реализовали параллельное выполнение env_runner-а, который собирает статистики, и процесса обновления весов. Это дало нам ускорение в 2.5 раза.**\n",
    "\n",
    "**Мы реализовали V-Trace, однако результаты с ним у нас пока были хуже, чем без него. Проблему пока не нашли.**\n",
    "\n",
    "### Планы на будущее\n",
    "\n",
    "**Понять в чем проблема с V-Trace**\n",
    "\n",
    "**Улучшить алгоритм обучения в данной среде за счет добавления других способов предсказания uncertainty агента для действий и наблюдений.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инструкции по запуску кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для запуска модели нужно установить нужные библиотеки, мы используем python3.6 и cuda9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установка torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.5.0+cu92 torchvision==0.6.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запуск агента с обученными весами\n",
    "\n",
    "1. Скачиваем веса из google drive с помощью wget (ниже) или руками: https://drive.google.com/drive/folders/15RBD-BrWUlylYLR13u7NWRhnMTAb9cWs?usp=sharing\n",
    "2. Пишем путь до весов в config.yaml. \n",
    "Пример: SavePath: \"/home/chervontsev/checkpoints/usual_2.ckpt\", выбираем надо ли оставлять UseStickyAction, мы по умолчанию оставляли. \n",
    "3. Запускаем eval.py скрипт: python eval.py. В логах скрипта будет: шаг с которого взят чекпоинт, ревард среды за игру, список посещенных комнат.\n",
    "4. Видео игры агента появится в папке MontezumaRevengeNoFrameskip-v4_example_run в корне проекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-28 21:33:17--  https://docs.google.com/uc?export=download&id=1XyO39st3QnCg5NnQ__OTH-ohwWA93YAx\n",
      "Resolving docs.google.com (docs.google.com)... 2a00:1450:4010:c01::c2, 64.233.162.194\n",
      "Connecting to docs.google.com (docs.google.com)|2a00:1450:4010:c01::c2|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-00-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ddul2fqhs1g7fjud52ah7n359rau0rap/1590690750000/01498080638632719806/*/1XyO39st3QnCg5NnQ__OTH-ohwWA93YAx?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-05-28 21:33:22--  https://doc-00-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ddul2fqhs1g7fjud52ah7n359rau0rap/1590690750000/01498080638632719806/*/1XyO39st3QnCg5NnQ__OTH-ohwWA93YAx?e=download\n",
      "Resolving doc-00-18-docs.googleusercontent.com (doc-00-18-docs.googleusercontent.com)... 2a00:1450:4010:c02::84, 74.125.205.132\n",
      "Connecting to doc-00-18-docs.googleusercontent.com (doc-00-18-docs.googleusercontent.com)|2a00:1450:4010:c02::84|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/octet-stream]\n",
      "Saving to: ‘usual_2.ckpt’\n",
      "\n",
      "usual_2.ckpt            [    <=>             ]  62.05M  76.5MB/s    in 0.8s    \n",
      "\n",
      "2020-05-28 21:33:23 (76.5 MB/s) - ‘usual_2.ckpt’ saved [65066732]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# usual_2.ckpt\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1XyO39st3QnCg5NnQ__OTH-ohwWA93YAx' -O usual_2.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-29 11:41:49--  https://docs.google.com/uc?export=download&id=1WdABk7j7Bs-1_rdp7wU6mfy6gnv5M0UO\n",
      "Resolving docs.google.com (docs.google.com)... 2a00:1450:4010:c0b::c2, 173.194.220.194\n",
      "Connecting to docs.google.com (docs.google.com)|2a00:1450:4010:c0b::c2|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0s-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/t0au88bc1na79l1a2c5l6vrie92a3l15/1590741675000/01498080638632719806/*/1WdABk7j7Bs-1_rdp7wU6mfy6gnv5M0UO?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-05-29 11:41:51--  https://doc-0s-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/t0au88bc1na79l1a2c5l6vrie92a3l15/1590741675000/01498080638632719806/*/1WdABk7j7Bs-1_rdp7wU6mfy6gnv5M0UO?e=download\n",
      "Resolving doc-0s-18-docs.googleusercontent.com (doc-0s-18-docs.googleusercontent.com)... 2a00:1450:4010:c09::84, 173.194.222.132\n",
      "Connecting to doc-0s-18-docs.googleusercontent.com (doc-0s-18-docs.googleusercontent.com)|2a00:1450:4010:c09::84|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/octet-stream]\n",
      "Saving to: ‘usual_1.ckpt’\n",
      "\n",
      "usual_1.ckpt            [    <=>             ]  62.55M  80.2MB/s    in 0.8s    \n",
      "\n",
      "2020-05-29 11:41:53 (80.2 MB/s) - ‘usual_1.ckpt’ saved [65593436]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# usual_1.ckpt\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1WdABk7j7Bs-1_rdp7wU6mfy6gnv5M0UO'\\\n",
    "    -O usual_1.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-28 21:42:39--  https://docs.google.com/uc?export=download&id=16UteZvE_4bcciNiCB4jK7EG_h1crHLeo\n",
      "Resolving docs.google.com (docs.google.com)... 2a00:1450:4010:c01::c2, 64.233.162.194\n",
      "Connecting to docs.google.com (docs.google.com)|2a00:1450:4010:c01::c2|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0k-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/q8k7o0c761eha360s6i831rlp319ermp/1590691350000/01498080638632719806/*/16UteZvE_4bcciNiCB4jK7EG_h1crHLeo?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-05-28 21:42:42--  https://doc-0k-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/q8k7o0c761eha360s6i831rlp319ermp/1590691350000/01498080638632719806/*/16UteZvE_4bcciNiCB4jK7EG_h1crHLeo?e=download\n",
      "Resolving doc-0k-18-docs.googleusercontent.com (doc-0k-18-docs.googleusercontent.com)... 2a00:1450:4010:c02::84, 74.125.205.132\n",
      "Connecting to doc-0k-18-docs.googleusercontent.com (doc-0k-18-docs.googleusercontent.com)|2a00:1450:4010:c02::84|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/octet-stream]\n",
      "Saving to: ‘only_intrinsic_fast.ckpt’\n",
      "\n",
      "only_intrinsic_fast     [    <=>             ]  47.40M  70.8MB/s    in 0.7s    \n",
      "\n",
      "2020-05-28 21:42:43 (70.8 MB/s) - ‘only_intrinsic_fast.ckpt’ saved [49702154]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# only_intrinsic_fast.ckpt\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=16UteZvE_4bcciNiCB4jK7EG_h1crHLeo'\\\n",
    "    -O only_intrinsic_fast.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_updates 5000\n",
      "Finished, total reward is 4600\n",
      "All visited rooms: {0, 1, 4, 10, 11, 18, 19, 20}\n"
     ]
    }
   ],
   "source": [
    "!python eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запуск обучения:\n",
    "\n",
    "1) Настраиваем config.yaml\n",
    "\n",
    "2) Делаем python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
